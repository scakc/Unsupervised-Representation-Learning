{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from scipy import misc\n",
    "import pickle\n",
    "import scipy.optimize as sopt\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output as clr\n",
    "np.random.seed(3038)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(directory):\n",
    "    print('Loading cifar10 Dataset...')\n",
    "    X_tr = []\n",
    "    X_te = None\n",
    "    Y_tr = []\n",
    "    Y_te = None\n",
    "\n",
    "    for i in range(5):\n",
    "        with open(directory+'/data_batch_' + str(i+1), 'rb') as file:\n",
    "            data_tr = pickle.load(file, encoding='latin1')\n",
    "            X_tr.append(np.reshape(data_tr['data'], [10000,3,32,32]))\n",
    "            Y_tr.append(np.array(data_tr['labels']))\n",
    "\n",
    "    with open(directory+'/test_batch', 'rb') as file:\n",
    "        data_te = pickle.load(file, encoding='latin1')\n",
    "        X_te = np.reshape(data_te['data'],[10000,3,32,32])\n",
    "        Y_te = np.array(data_te['labels'])\n",
    "\n",
    "    X = np.concatenate(X_tr, axis = 0).astype(np.float)\n",
    "    X = np.swapaxes(X, 1, 3)\n",
    "    X_train = np.swapaxes(X, 1, 2)\n",
    "    Y_train = np.concatenate(Y_tr, axis = 0).astype(np.int)\n",
    "\n",
    "    X = np.swapaxes(X_te, 1, 3)\n",
    "    X_test = np.swapaxes(X, 1, 2).astype(np.float)\n",
    "    Y_test = Y_te.astype(np.int)\n",
    "\n",
    "    print('Done ! Loaded cifar10 dataset')\n",
    "    return X_train/255, Y_train, X_test/255, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = 'Abhishek Kumar'\n",
    "        print('Abhishek is here to help')\n",
    "        \n",
    "    def sample_targets(self, N, Z):\n",
    "        '''\n",
    "        N : number of targets to be sampled\n",
    "        Z : dimenstionality of unit sphere\n",
    "        '''\n",
    "\n",
    "        samples = np.random.normal(0,1, [N,Z]).astype(np.float32)\n",
    "        magnitudes = np.expand_dims(np.sqrt(np.sum(np.square(samples),axis=1)),1)\n",
    "\n",
    "        return samples/magnitudes\n",
    "\n",
    "    def get_nearest_targets(self, samples, targets):\n",
    "        print('Getting optimal assignments of labels..')\n",
    "        to_optimize = np.zeros([samples.shape[0], targets.shape[0]])\n",
    "        for i in range(samples.shape[0]):\n",
    "            to_optimize[:,i] = np.sum(np.square(samples-targets[i,:]), axis = 1)\n",
    "\n",
    "        _, ind_assigns = sopt.linear_sum_assignment(to_optimize)\n",
    "        return targets[ind_assigns]\n",
    "\n",
    "    def shuffle_unision(self, nd1, nd2):\n",
    "        assert nd1.shape[0] == nd2.shape[0], 'Arrays first dimesnsions should of same size' \n",
    "        permutation = np.random.permutation(nd1.shape[0])\n",
    "        return nd1[permutation], nd2[permutation]\n",
    "\n",
    "    def get_collage(self, images, size):\n",
    "        h, w = images.shape[1], images.shape[2]\n",
    "        print(images.shape)\n",
    "        img = np.zeros((h * size[0], w * size[1], 3))\n",
    "\n",
    "        for idx, image in enumerate(images[0:size[0]*size[1], :, :, :]):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FB_NAT:\n",
    "    \n",
    "    # https://github.com/mrjel/noise-as-targets-tensorflow/blob/master/model.py\n",
    "    def __init__(self, args, input_shape = [32,32,3]):\n",
    "        \n",
    "        #Assuming for rgb image data\n",
    "        W,H,C = input_shape\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Placeholders\n",
    "        self.args = args\n",
    "        self.input_ = tf.placeholder(dtype=tf.float32, shape=[None, W, H, C]) # input_train_ph\n",
    "        self.target_ = tf.placeholder(dtype=tf.float32, shape=[None, self.args['latent_dim']]) # targets\n",
    "        self.drop_rate_ = tf.placeholder(tf.float32) # dropout_keep_prob\n",
    "        self.initial_lr_ = tf.placeholder(tf.float32) # new_lr\n",
    "        \n",
    "        # Variables\n",
    "        self.lr = tf.Variable(self.args['lr'])\n",
    "        \n",
    "        # Preprocessings\n",
    "        self.input_tr = self.preprocess(self.input_)\n",
    "        \n",
    "        # Model Definition\n",
    "        self.latent_vec = self.encoder_network(self.input_tr)\n",
    "        \n",
    "        # Classifier\n",
    "        self.c_labels_ = tf.placeholder(dtype=tf.int32, shape=[None], name = 'classfier_labels')\n",
    "        self.initial_c_lr_ = tf.placeholder(tf.float32)\n",
    "        self.c_lr = tf.Variable(self.args['c_lr'])\n",
    "        \n",
    "        self.logits_ = self.classifier(self.latent_vec)\n",
    "        self.cla_top_k = tf.nn.in_top_k(self.logits_,self.c_labels_,1)\n",
    "        \n",
    "        \n",
    "        # Objective Losses\n",
    "        self.loss = self.get_loss()\n",
    "        self.cla_loss = self.get_cla_loss()\n",
    "        \n",
    "        # Trainings\n",
    "        self.train = self.train_ret()\n",
    "        self.cla_train = self.cla_train_ret()\n",
    "        \n",
    "        # update lr\n",
    "        self.update_lr = tf.assign(self.lr, self.initial_lr_, name='lr_update')\n",
    "        self.update_lr_cla = tf.assign(self.c_lr, self.initial_c_lr_, name='cla_lr_update')\n",
    "    \n",
    "    \n",
    "    def encoder_network(self, x, reuse_variables = False, d_format = 'channels_last'):\n",
    "        with tf.variable_scope(\"enc\", reuse = reuse_variables) as enc_vs:\n",
    "            latent_dim = self.args['latent_dim']\n",
    "            num_layers = 4\n",
    "            hid_c = 64\n",
    "            # Deep Network\n",
    "            x = tf.layers.conv2d(x, hid_c, 3, 1, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c, 3, 1, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c, 3, 1, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c, 3, 2, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c*2, 3, 1, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c*2, 3, 1, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c*2, 3, 2, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c*3, 3, 1, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c*3, 3, 1, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c*3, 3, 2, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c*4, 3, 1, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            x = tf.layers.conv2d(x, hid_c*4, 3, 1, padding='same', activation=tf.nn.elu, data_format=d_format)\n",
    "            \n",
    "            # Flattened layers\n",
    "            x = tf.layers.flatten(x,name=None,data_format=d_format)\n",
    "            x = tf.nn.dropout(x,self.drop_rate_)\n",
    "            x = tf.layers.dense(x, hid_c*4, activation=tf.nn.elu)\n",
    "            x = tf.nn.dropout(x,self.drop_rate_)\n",
    "            x = tf.layers.dense(x, latent_dim, activation=None)\n",
    "            z = tf.nn.l2_normalize(x,1)\n",
    "            \n",
    "        self.enc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='enc')\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def classifier(self, x, reuse_variables = False):\n",
    "        with tf.variable_scope(\"cla\",reuse=reuse_variables) as cla_vs:\n",
    "\n",
    "            x = tf.layers.dense(x, self.args['n_classes']*20, activation=tf.nn.relu)\n",
    "            x = tf.layers.dense(x, self.args['n_classes']*20, activation=tf.nn.relu)\n",
    "            x = tf.layers.dense(x, self.args['n_classes'], activation=None, name = 'classfier_logits')\n",
    "\n",
    "        self.cla_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='cla')\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def preprocess(self, input_):\n",
    "        \n",
    "        # Augmenting the data\n",
    "        W,H,C = self.input_shape\n",
    "        \n",
    "        input_ = tf.map_fn(lambda img: tf.image.flip_left_right(img), input_)\n",
    "        input_ = tf.map_fn(lambda img: tf.image.random_brightness(img,max_delta=63), input_)\n",
    "        input_ = tf.map_fn(lambda img: tf.image.random_contrast(img, lower=0.2, upper=1.8),input_)\n",
    "        input_ = tf.map_fn(lambda img: tf.image.per_image_standardization(img),input_)\n",
    "        input_ = tf.map_fn(lambda img: tf.image.resize_image_with_crop_or_pad(img,int(H*(30/32)),int(H*(30/32))),input_)\n",
    "        input_ = tf.map_fn(lambda img: tf.image.resize_image_with_crop_or_pad(img,int(W*(42/32)),int(H*(42/32))),input_)\n",
    "        input_ = tf.map_fn(lambda img: tf.random_crop(img,[W,H,C]),input_)\n",
    "        \n",
    "        return input_\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def cla_train_ret(self):\n",
    "        optimizer = tf.train.AdamOptimizer(self.c_lr)\n",
    "        cla_train = optimizer.minimize(self.cla_loss, var_list = self.cla_vars)\n",
    "        return cla_train\n",
    "    \n",
    "        \n",
    "    def train_ret(self):\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        enc_train = optimizer.minimize(self.loss, var_list = self.enc_vars)\n",
    "        return enc_train\n",
    "    \n",
    "    def get_cla_loss(self):\n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits = self.logits_,labels = self.c_labels_))\n",
    "    \n",
    "    def get_loss(self):\n",
    "        return tf.reduce_mean(self.l2_sqr(self.target_, self.latent_vec))\n",
    "    \n",
    "    \n",
    "    def l2_sqr(self, x, y):\n",
    "        return tf.reduce_sum(tf.square(x - y),axis=1)\n",
    "    \n",
    "    def l1(self, x, y):\n",
    "        return tf.reduce_sum(tf.abs(x - y),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abhishek is here to help\n"
     ]
    }
   ],
   "source": [
    "args = dict(\n",
    "    input_type='cifar_10',\n",
    "    n_epochs = 200,\n",
    "    lr = 0.0001,\n",
    "    c_lr = 0.001,\n",
    "    batch_size = 256,\n",
    "    model_dir = '../models/NAT_FB',\n",
    "    data_dir = '../data/cifar-10-batches-py',\n",
    "    print_interval = 2,\n",
    "    c_train_interval = 10,\n",
    "    c_epochs = 1,\n",
    "    latent_dim = 32,\n",
    "    n_classes = 10\n",
    ")\n",
    "helper = Helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cifar10 Dataset...\n",
      "Done ! Loaded cifar10 dataset\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_cifar10(args['data_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0816 22:36:24.058589 140480208467776 deprecation.py:323] From /home/abhi/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0816 22:36:24.193047 140480208467776 deprecation.py:323] From <ipython-input-4-d38d3f136652>:54: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0816 22:36:24.196677 140480208467776 deprecation.py:506] From /home/abhi/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0816 22:36:24.549981 140480208467776 deprecation.py:323] From <ipython-input-4-d38d3f136652>:68: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0816 22:36:24.750996 140480208467776 deprecation.py:506] From <ipython-input-4-d38d3f136652>:69: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0816 22:36:24.762318 140480208467776 deprecation.py:323] From <ipython-input-4-d38d3f136652>:70: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0816 22:36:25.203532 140480208467776 deprecation.py:323] From /home/abhi/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "encoder = FB_NAT(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier init\n",
    "cla_initializer = tf.variables_initializer(encoder.cla_vars)\n",
    "enc_initializer = tf.variables_initializer(encoder.enc_vars)\n",
    "global_initializer = tf.initializers.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model saver \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = helper.sample_targets(X_train.shape[0], args['latent_dim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training The model\n",
    "train_loss = []\n",
    "cla_loss = []\n",
    "data_train_X = X_train.copy()\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0.6051282051282051/195, Loss: 1.4589\n",
      "Train_loss_5 [1.460522, 1.4326866, 1.4792104, 1.444942, 1.4644113] \n",
      " Classifier_loss_5 [1.9893492, 1.996409, 2.021879, 1.9884981, 1.9273882]\n",
      "Getting optimal assignments of labels..\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(global_initializer)\n",
    "    \n",
    "    batch_size = args['batch_size']\n",
    "    num_batches = X_train.shape[0]//batch_size\n",
    "    curr_epoch = -1\n",
    "    for counter in range(args['n_epochs']*num_batches):\n",
    "        if(counter%num_batches == 0):\n",
    "            batch_ind = 0\n",
    "            curr_epoch += 1\n",
    "            \n",
    "            target, data_train_X = helper.shuffle_unision(target, data_train_X)\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Getting the current set of bacthes \n",
    "        batch_x = data_train_X[batch_ind:batch_ind + batch_size]\n",
    "        batch_y = target[batch_ind:batch_ind + batch_size]\n",
    "        \n",
    "        # Recalc the optimal assigments\n",
    "        if(curr_epoch%3 == 0):            \n",
    "            feed_dict_ = {encoder.input_:batch_x, encoder.drop_rate_:1.0}\n",
    "            z = sess.run(encoder.latent_vec, feed_dict = feed_dict_)\n",
    "            \n",
    "            # Using Hungsess.run(tf.variables_initializer(encoder.enc_vars))arain Algorithm of Scipy\n",
    "            batch_y = helper.get_nearest_targets(z, batch_y)\n",
    "            target[batch_ind:batch_ind + batch_size] = batch_y\n",
    "        \n",
    "        # Training the network\n",
    "        feed_dict_ = {encoder.target_: batch_y, encoder.input_: batch_x,encoder.drop_rate_:0.5}\n",
    "        fetch_dict = {\n",
    "                \"train\": encoder.train\n",
    "        }\n",
    "        \n",
    "        if(counter%args['print_interval'] == 0):\n",
    "            fetch_dict.update({\n",
    "                \"loss\" : encoder.loss\n",
    "            })\n",
    "            \n",
    "            train_loss.append(loss)\n",
    "        \n",
    "        result = sess.run(fetch_dict, feed_dict = feed_dict_)\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        if(counter%args['print_interval'] == 0):\n",
    "            \n",
    "            clr(wait = True)\n",
    "            loss = result['loss']\n",
    "            print(\"Epoch: {}, Batch: {}/{}, Loss: {:.4f}\". \\\n",
    "                  format(curr_epoch, counter/num_batches, num_batches, loss))\n",
    "            \n",
    "            print('Train_loss_5', train_loss[-5:], '\\n Classifier_loss_5', cla_loss[-5:])\n",
    "        \n",
    "        # Training Classifier\n",
    "        if(curr_epoch%args['c_train_interval'] == 0 and counter/num_batches == 0):\n",
    "            # ReInitialize\n",
    "            sess.run(cla_initializer)\n",
    "            cla_loss = []\n",
    "            \n",
    "            # Compute Vecs\n",
    "            cal_batch_ind = 0\n",
    "            perm = np.random.permutation(X_train.shape[0])\n",
    "#             labels, inputs = helper.shuffle_unision(Y_train, X_train)\n",
    "            \n",
    "            for i in range(args['c_epochs']*num_batches):\n",
    "                \n",
    "                if(i%num_batches == 0):\n",
    "                    i = 0\n",
    "            \n",
    "                batch_labels = Y_train[perm[i*batch_size:(i+1)*batch_size]]\n",
    "                batch_inputs = X_train[perm[i*batch_size:(i+1)*batch_size]]\n",
    "                \n",
    "                _, loss, top_k = sess.run(\n",
    "                    [encoder.cla_train,encoder.cla_loss, encoder.cla_top_k],\n",
    "                         feed_dict={encoder.c_labels_:batch_labels, encoder.input_: batch_inputs,\n",
    "                                    encoder.drop_rate_:1.0})\n",
    "                \n",
    "                top_k = np.sum(top_k)\n",
    "                \n",
    "                cla_loss.append(loss)\n",
    "                \n",
    "                if i % 30 == 0:\n",
    "                    print(\"Classifier: Batch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}\". \\\n",
    "                              format(i,num_batches, loss, float(top_k)/batch_size))\n",
    "                \n",
    "                \n",
    "                \n",
    "        counter += 1\n",
    "        batch_ind += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
